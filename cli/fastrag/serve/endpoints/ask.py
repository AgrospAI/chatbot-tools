import json

from fastapi import APIRouter, Depends, Path
from fastapi.responses import StreamingResponse
from pydantic import BaseModel

from fastrag.serve.db import get_chat_repository, init_db

from ..dependencies import (
    get_config,
    get_embedding_model,
    get_llm,
    get_vector_store,
)

router = APIRouter()
init_db()  # Ensure DB is initialized on import
chat_repo = get_chat_repository()


class QuestionRequest(BaseModel):
    question: str


def build_prompt(context: str, question: str) -> str:
    return f"""
    You are a helpful assistant and expert in data spaces.

    Always use inline references in the form [<NUMBER OF DOCUMENT>](ref:<NUMBER OF DOCUMENT>)
    ONLY if you use information from a document. For example, if you use the information from
    Document[3], you should write [3](ref:3) at the end of the sentence where you used that
    information.
    Give a precise, accurate and structured answer without repeating the question.
    
    These are the documents:
    {context}

    Question:
    {question}

    Answer:
    """.strip()


@router.post("/ask/{chat_id}")
async def ask_question(
    chat_id: str = Path(..., description="Chat session id generated by the client"),
    req: QuestionRequest = None,
    vector_store=Depends(get_vector_store),
    llm=Depends(get_llm),
    config=Depends(get_config),
    embedding_model=Depends(get_embedding_model),
) -> StreamingResponse:
    # # Save user message
    chat_repo.save_message(chat_id=chat_id, content=req.question, role="user")

    query_embedding = await embedding_model.embed_query(req.question)

    results = await vector_store.similarity_search(
        query=req.question, query_embedding=query_embedding, k=5
    )
    context_parts = [f"Document[{i}]: {doc.page_content}" for i, doc in enumerate(results)]
    context = "\n\n".join(context_parts)
    sources_metadata = [doc.metadata.get("source") for doc in results]

    async def generate():
        yield f"data: {json.dumps({'type': 'sources', 'data': sources_metadata})}\n\n"
        prompt = build_prompt(context, req.question)
        
        answer = ""
        async for token in llm.stream(prompt):
            answer += token
            yield f"data: {json.dumps({'type': 'token', 'data': token})}\n\n"
        chat_repo.save_message(
            chat_id=chat_id, content=answer, role="assistant", sources=sources_metadata
        )

    return StreamingResponse(
        content=generate(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no",
        },
    )
