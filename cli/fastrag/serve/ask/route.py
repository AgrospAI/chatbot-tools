import json
import time

from fastapi import APIRouter, Depends, Path, Request
from fastapi.responses import StreamingResponse
from langchain_core.embeddings import Embeddings
from pydantic import BaseModel

from fastrag.llms.llm import ILLM
from fastrag.serve.ask.dependencies import get_embedding_model, get_llm, get_vector_store
from fastrag.serve.chats.service import ChatService, get_chat_service
from fastrag.serve.geolocalization import get_country_from_ip
from fastrag.serve.rate_limiting import limiter
from fastrag.serve.telemetry.metrics import (
    http_request_errors_total,
    http_requests_in_flight,
    http_requests_total,
    llm_answer_length,
    llm_question_length,
    llm_time_to_first_token,
    llm_time_to_last_token,
    requests_per_ip_total,
)
from fastrag.stores.store import IVectorStore

AskRouter = APIRouter()


class QuestionRequest(BaseModel):
    question: str


def build_prompt(context: str, question: str) -> str:
    return f"""
    You are a helpful assistant and expert in data spaces.

    Always use inline references in the form [<NUMBER OF DOCUMENT>](ref:<NUMBER OF DOCUMENT>)
    ONLY if you use information from a document. For example, if you use the information from
    Document[3], you should write [3](ref:3) at the end of the sentence where you used that
    information.
    Give a precise, accurate and structured answer without repeating the question.
    
    These are the documents:
    {context}

    Question:
    {question}

    Answer:
    """.strip()


@AskRouter.post("/ask/{chat_id}")
@limiter.limit("5/minute")
async def ask_question(
    chat_id: str = Path(..., description="Chat session id generated by the client"),
    request: Request = None,
    req: QuestionRequest = None,
    vector_store: IVectorStore = Depends(get_vector_store),
    llm: ILLM = Depends(get_llm),
    embedding_model: Embeddings = Depends(get_embedding_model),
    chatService: ChatService = Depends(get_chat_service),
) -> StreamingResponse:
    path = "/ask/{chat_id}"
    method = "POST"
    client_ip = request.client.host

    http_requests_in_flight.add(1, {"path": path})
    requests_per_ip_total.add(1, {"ip": client_ip})
    llm_question_length.record(len(req.question), {})

    # Get country from IP
    country = get_country_from_ip(client_ip)

    # Save user message (this will create the chat with IP and country if it doesn't exist)
    chatService.save_message(
        chat_id=chat_id, content=req.question, role="user", ip=client_ip, country=country
    )

    query_embedding = await embedding_model.aembed_query(req.question)

    results = await vector_store.similarity_search(
        query=req.question, query_embedding=query_embedding, k=5
    )
    context_parts = [f"Document[{i}]: {doc.page_content}" for i, doc in enumerate(results)]
    context = "\n\n".join(context_parts)
    sources_metadata = [doc.metadata.get("source") for doc in results]

    start_time = time.monotonic()
    first_token_time = None

    answer = ""

    try:

        async def generate():
            yield f"data: {json.dumps({'type': 'sources', 'data': sources_metadata})}\n\n"
            nonlocal first_token_time, answer
            prompt = build_prompt(context, req.question)

            async for token in llm.stream(prompt):
                if first_token_time is None:
                    first_token_time = time.monotonic()
                    llm_time_to_first_token.record(first_token_time - start_time, {})
                answer += token
                yield f"data: {json.dumps({'type': 'token', 'data': token})}\n\n"
            chatService.save_message(
                chat_id=chat_id, content=answer, role="assistant", sources=sources_metadata
            )

        return StreamingResponse(
            content=generate(),
            media_type="text/event-stream",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
                "X-Accel-Buffering": "no",
            },
        )
    except Exception as e:
        http_request_errors_total.add(1, {"path": path, "type": type(e).__name__})
        raise
    finally:
        http_requests_in_flight.add(-1, {"path": path})

        if first_token_time:
            llm_time_to_last_token.record(time.monotonic() - start_time, {})
            llm_answer_length.record(len(answer), {})

        http_requests_total.add(1, {"method": method, "path": path, "status": "200"})
